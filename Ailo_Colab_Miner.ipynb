{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# üöÄ Ailo Colab Miner (AILO-1B)\n\n**Mine AiloCoin with Google Colab's free T4 GPU!**\n\n1. Runtime > Change runtime type > T4 GPU\n2. Enter wallet below\n3. Runtime > Run all\n\n‚ú® **NEW**: Now syncs with global model!"
            ],
            "metadata": {
                "id": "h"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "#@title ‚öôÔ∏è 1. Setup\n!pip install torch --index-url https://download.pytorch.org/whl/cu118 -q\n!pip install aiohttp requests -q\nimport gc; gc.collect()\nimport torch\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)} | VRAM: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f}GB\")\nelse:\n    print(\"‚ùå Enable GPU: Runtime > Change runtime type > T4\")"
            ],
            "metadata": {
                "id": "s"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title üîë 2. Wallet\nWALLET = \"\"  #@param {type:\"string\"}\nprint(f\"‚úÖ {WALLET[:12]}...\" if len(WALLET)>=40 else \"‚ùå Enter wallet from https://ailo.site/wallet.html\")"
            ],
            "metadata": {
                "id": "w"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title üß† 3. AILO-1B Model + Global Sync\nimport torch, torch.nn as nn, numpy as np, requests, aiohttp, asyncio, time, base64, gzip, gc, os\n\nAPI=\"https://ailo.site/api\"\nSERVER=\"https://ailo.site\"\nVER=\"1.5.0-colab\"\n\nclass AILO1B(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.emb=nn.Embedding(50257,1600)\n        self.pos=nn.Parameter(torch.zeros(1,512,1600))\n        self.tf=nn.TransformerEncoder(nn.TransformerEncoderLayer(1600,25,6400,0.1,batch_first=True),24)\n        self.out=nn.Linear(1600,50257)\n    def forward(self,x):\n        return self.out(self.tf(self.emb(x)*40+self.pos[:,:x.size(1)]))\n\nclass Trainer:\n    def __init__(self):\n        self.dev=torch.device('cuda')\n        self.m=None; self.opt=None; self.steps=0; self.acc=0\n        self.local_version=0\n    \n    def init(self):\n        print(\"üß† Loading AILO-1B...\")\n        gc.collect(); torch.cuda.empty_cache()\n        self.m=AILO1B().half().to(self.dev)\n        print(f\"   {sum(p.numel() for p in self.m.parameters()):,} params | GPU: {torch.cuda.memory_allocated()/1e9:.1f}GB\")\n        self.opt=torch.optim.SGD(self.m.parameters(),lr=0.001,momentum=0.9)\n        self.crit=nn.CrossEntropyLoss()\n        print(\"   ‚úÖ Ready!\")\n    \n    def sync_with_global(self):\n        \"\"\"Sync with global aggregated gradients from server.\"\"\"\n        try:\n            print(\"üîÑ Checking for global model updates...\")\n            r=requests.get(f\"{SERVER}/api/model/weights\",timeout=30)\n            if r.status_code!=200:\n                print(\"   No global model yet, using local\")\n                return False\n            \n            data=r.json()\n            srv_ver=data.get('version',0)\n            \n            if srv_ver<=self.local_version:\n                print(f\"   Already at v{self.local_version}\")\n                return True\n            \n            print(f\"   Server has v{srv_ver}, local has v{self.local_version}\")\n            \n            if data.get('downloadUrls',{}).get('gradients'):\n                grad_url=f\"{SERVER}{data['downloadUrls']['gradients']}\"\n                print(f\"   üì• Downloading gradients v{srv_ver}...\")\n                gr=requests.get(grad_url,timeout=60)\n                if gr.status_code==200:\n                    self._apply_gradients(gr.content)\n                    self.local_version=srv_ver\n                    print(f\"   ‚úÖ Synced to v{srv_ver}\")\n                    return True\n            return False\n        except Exception as e:\n            print(f\"   ‚ö†Ô∏è Sync failed: {e}\")\n            return False\n    \n    def _apply_gradients(self, grad_bytes):\n        \"\"\"Apply aggregated gradients to output layer.\"\"\"\n        try:\n            grads=np.frombuffer(grad_bytes,dtype=np.float16).astype(np.float32)\n            print(f\"   Received {len(grads):,} gradient values\")\n            with torch.no_grad():\n                out_params=self.m.out.weight.numel()\n                if len(grads)>=out_params:\n                    lr=0.0001\n                    gt=torch.from_numpy(grads[:out_params]).reshape_as(self.m.out.weight)\n                    self.m.out.weight.data-=lr*gt.half().to(self.dev)\n                    print(f\"   Applied to output layer ({out_params:,} params)\")\n        except Exception as e:\n            print(f\"   ‚ö†Ô∏è Apply failed: {e}\")\n    \n    def batch(self,w):\n        try:\n            r=requests.get(f\"{API}/cuda/training-data\",params={'batchSize':1,'wallet':w},timeout=10)\n            if r.ok:\n                t=r.json().get('articles',[''])[0]\n                tk=[ord(c)%50257 for c in t[:33]]\n                tk+=[0]*(33-len(tk))\n                return torch.tensor([tk[:32]]),torch.tensor([tk[1:33]])\n        except:pass\n        return torch.randint(0,50257,(1,32)),torch.randint(0,50257,(1,32))\n    \n    def step(self,x,y):\n        self.m.train()\n        x,y=x.to(self.dev),y.to(self.dev)\n        with torch.cuda.amp.autocast():\n            loss=self.crit(self.m(x).view(-1,50257),y.view(-1))/16\n        loss.backward()\n        self.acc+=1\n        if self.acc>=16:\n            nn.utils.clip_grad_norm_(self.m.parameters(),1.0)\n            self.opt.step(); self.opt.zero_grad(set_to_none=True); self.acc=0\n        self.steps+=1\n        if self.steps%50==0: gc.collect(); torch.cuda.empty_cache()\n        return loss.item()*16\n    \n    def grads(self):\n        g=self.m.out.weight.grad\n        if g is None: return None\n        gc.collect()\n        flat=g.cpu().float().flatten()\n        sample=flat[::10]\n        del flat; gc.collect()\n        comp=gzip.compress(sample.half().numpy().tobytes(),4)\n        del sample; gc.collect()\n        return base64.b64encode(comp).decode()\n\nprint(\"‚úÖ Ready (with global sync)\")"
            ],
            "metadata": {
                "id": "m"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title üöÄ 4. Mine!\n\nasync def hb(w,tps):\n    try:\n        async with aiohttp.ClientSession() as s:\n            await s.post(f\"{API}/cuda/register\",json={'wallet':w,'clientVersion':VER,'deviceInfo':{'gpu_name':'T4-Colab','vram_gb':15,'hashrate':tps}})\n            await s.post(f\"{API}/ping\",json={'wallet':w,'status':'training','hashrate':f\"{tps:.0f} Tok/s\"})\n    except:pass\n\nasync def submit(w,g,s,l):\n    try:\n        async with aiohttp.ClientSession() as ss:\n            async with ss.post(f\"{API}/cuda/submit\",json={'wallet':w,'gradients':g,'epoch':s,'loss':l,'gpu':'T4-Colab'}) as r:\n                if r.status==200: return (await r.json()).get('reward',0)\n    except:pass\n    return 0\n\nasync def run():\n    if len(WALLET)<40: print(\"‚ùå Set wallet!\"); return\n    print(\"=\"*50)\n    print(f\"  AILO-1B Colab Miner v{VER}\")\n    print(\"  üîÑ Now with Global Model Sync!\")\n    print(\"=\"*50)\n    \n    await hb(WALLET,0)\n    t=Trainer(); t.init()\n    \n    # Initial sync with global model\n    t.sync_with_global()\n    \n    print(\"\\n‚õèÔ∏è MINING!\")\n    print(\"üìä Dashboard: https://ailo.site/dashboard.html\\n\")\n    \n    rew,last_s,last_h,last_sync,tps=0.0,time.time(),time.time(),time.time(),0.0\n    \n    try:\n        while True:\n            t0=time.time()\n            x,y=t.batch(WALLET)\n            loss=t.step(x,y)\n            tps=32/(time.time()-t0+0.001)\n            \n            if time.time()-last_h>=10: await hb(WALLET,tps); last_h=time.time()\n            if t.steps%25==0: print(f\"Step {t.steps} | Loss: {loss:.4f} | {tps:.0f} tok/s | v{t.local_version} | GPU: {torch.cuda.memory_allocated()/1e9:.1f}GB\")\n            \n            if time.time()-last_s>=300:\n                print(\"\\nüì§ Submitting gradients...\")\n                gc.collect(); torch.cuda.empty_cache()\n                g=t.grads()\n                if g:\n                    r=await submit(WALLET,g,t.steps,loss); rew+=r\n                    print(f\"üí∞ +{r:.4f} ALC (Total: {rew:.4f})\")\n                last_s=time.time()\n                \n                # Sync every 10 minutes\n                if time.time()-last_sync>=600:\n                    t.sync_with_global()\n                    last_sync=time.time()\n                print()\n                \n    except KeyboardInterrupt: print(f\"\\n‚èπÔ∏è Total: {rew:.4f} ALC\")\n\nawait run()"
            ],
            "metadata": {
                "id": "r"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}