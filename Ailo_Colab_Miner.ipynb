{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "#@title ðŸš€ Ailo Colab Miner v2.0 - Full Model Training!\n#@markdown ### Instructions:\n#@markdown 1. Runtime > Change runtime type > **T4 GPU**\n#@markdown 2. Enter your wallet address below\n#@markdown 3. Press **â–¶ï¸ Play** - that's it!\n#@markdown ---\n\nWALLET = \"\"  #@param {type:\"string\"}\n\n# ===== AILO COLAB MINER v2.0 - FULL MODEL =====\nimport subprocess, sys\nsubprocess.run([sys.executable, '-m', 'pip', 'install', 'torch', '--index-url', 'https://download.pytorch.org/whl/cu118', '-q'], capture_output=True)\nsubprocess.run([sys.executable, '-m', 'pip', 'install', 'aiohttp', 'requests', '-q'], capture_output=True)\n\nimport torch, torch.nn as nn, numpy as np, requests, aiohttp, asyncio, time, base64, gzip, gc, json, io\n\nAPI, SERVER, VER = \"https://ailo.site/api\", \"https://ailo.site\", \"2.0.0-colab-full\"\nMAX_LOSS = 2.8\n\nif not torch.cuda.is_available():\n    print(\"âŒ GPU not enabled! Go to: Runtime > Change runtime type > T4 GPU\")\n    raise SystemExit\n\nif len(WALLET) < 40:\n    print(\"âŒ Enter your wallet address above! Get one at: https://ailo.site/wallet.html\")\n    raise SystemExit\n\nvram = torch.cuda.get_device_properties(0).total_memory/1e9\nprint(f\"âœ… GPU: {torch.cuda.get_device_name(0)} | VRAM: {vram:.1f}GB\")\nprint(f\"âœ… Wallet: {WALLET[:12]}...\")\n\nclass AILO1B(nn.Module):\n    \"\"\"AILO-1B: 899M parameter model\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.emb = nn.Embedding(50257, 1600)\n        self.pos = nn.Parameter(torch.zeros(1, 512, 1600))\n        self.tf = nn.TransformerEncoder(nn.TransformerEncoderLayer(1600, 25, 6400, 0.1, batch_first=True), 24)\n        self.out = nn.Linear(1600, 50257)\n    def forward(self, x):\n        return self.out(self.tf(self.emb(x) * 40 + self.pos[:, :x.size(1)]))\n\nclass FullModelTrainer:\n    \"\"\"Trainer for full 899M model\"\"\"\n    def __init__(self):\n        self.dev = torch.device('cuda')\n        self.m = None; self.opt = None; self.steps = 0; self.acc = 0\n        self.best_loss = float('inf')\n        self.saved_grads = {}  # All layer gradients\n        self.synced = False\n        self.layer_info = None\n    \n    def init(self):\n        print(\"ðŸ§  Loading AILO-1B (899M params)...\")\n        gc.collect(); torch.cuda.empty_cache()\n        self.m = AILO1B().half().to(self.dev)\n        params = sum(p.numel() for p in self.m.parameters())\n        print(f\"   âœ… Ready! {params:,} params | GPU Memory: {torch.cuda.memory_allocated()/1e9:.1f}GB\")\n        self.opt = torch.optim.AdamW(self.m.parameters(), lr=1e-4, weight_decay=0.01)\n        self.crit = nn.CrossEntropyLoss()\n    \n    def sync_full_model(self):\n        \"\"\"Download and load full 1.8GB checkpoint\"\"\"\n        try:\n            print(\"ðŸ”„ Syncing full model (1.8GB)...\")\n            \n            # Get model info\n            r = requests.get(f\"{SERVER}/api/model/weights\", timeout=30)\n            if r.status_code != 200: \n                print(\"   âš ï¸ No model info available\")\n                return False\n            \n            data = r.json()\n            server_loss = data.get('bestCheckpointLoss', 999)\n            \n            # Try full checkpoint first (1.8GB)\n            print(f\"   ðŸ“¥ Downloading full checkpoint (loss: {server_loss})...\")\n            cr = requests.get(f\"{SERVER}/api/model/checkpoint/full\", timeout=300, stream=True)\n            \n            if cr.status_code == 200:\n                # Load full checkpoint\n                weights = np.frombuffer(cr.content, dtype=np.float16)\n                print(f\"   ðŸ“¦ Loaded {len(weights):,} params ({len(weights)*2/1e9:.2f}GB)\")\n                \n                # Get layer info\n                info_r = requests.get(f\"{SERVER}/api/model/checkpoint/full/info\", timeout=10)\n                if info_r.status_code == 200:\n                    self.layer_info = info_r.json().get('layers', [])\n                    \n                    # Load each layer\n                    with torch.no_grad():\n                        for layer in self.layer_info:\n                            name = layer['name']\n                            offset = layer['offset']\n                            size = layer['size']\n                            shape = layer['shape']\n                            \n                            # Get the parameter\n                            parts = name.split('.')\n                            param = self.m\n                            for p in parts:\n                                if p.isdigit():\n                                    param = param[int(p)]\n                                else:\n                                    param = getattr(param, p, None)\n                                    if param is None: break\n                            \n                            if param is not None and hasattr(param, 'data'):\n                                w = weights[offset:offset+size].astype(np.float32)\n                                param.data = torch.from_numpy(w).reshape(shape).half().to(self.dev)\n                    \n                    self.synced = True\n                    self.best_loss = float(server_loss) if isinstance(server_loss, (int, float)) else 2.8\n                    print(f\"   âœ… Full model loaded! Target loss: {self.best_loss:.4f}\")\n                    return True\n            \n            # Fallback to partial checkpoint\n            print(\"   âš ï¸ Full checkpoint not available, trying partial...\")\n            cr = requests.get(f\"{SERVER}/api/model/checkpoint\", timeout=60)\n            if cr.status_code == 200:\n                w = np.frombuffer(cr.content, dtype=np.float16).astype(np.float32)\n                with torch.no_grad():\n                    exp = self.m.out.weight.numel()\n                    if len(w) >= exp:\n                        self.m.out.weight.data = torch.from_numpy(w[:exp]).reshape_as(self.m.out.weight).half().to(self.dev)\n                        self.synced = True\n                        self.best_loss = float(server_loss) if isinstance(server_loss, (int, float)) else 2.8\n                        print(f\"   âœ… Partial model loaded (out.weight only)\")\n                return True\n            return False\n        except Exception as e:\n            print(f\"   âš ï¸ Sync failed: {e}\")\n            return False\n    \n    def batch(self, w):\n        try:\n            r = requests.get(f\"{API}/cuda/training-data\", params={'batchSize': 1, 'wallet': w}, timeout=10)\n            if r.ok:\n                t = r.json().get('articles', [''])[0]\n                if len(t) >= 33:\n                    tk = [ord(c) % 50257 for c in t[:33]]\n                    return torch.tensor([tk[:32]]), torch.tensor([tk[1:33]])\n        except: pass\n        return None, None\n    \n    def step(self, x, y):\n        if x is None: return None\n        self.m.train()\n        x, y = x.to(self.dev), y.to(self.dev)\n        with torch.cuda.amp.autocast():\n            loss = self.crit(self.m(x).view(-1, 50257), y.view(-1)) / 16\n        loss.backward()\n        self.acc += 1\n        if self.acc >= 16:\n            nn.utils.clip_grad_norm_(self.m.parameters(), 1.0)\n            # Save ALL gradients (sampled)\n            for name, p in self.m.named_parameters():\n                if p.grad is not None:\n                    # Sample 1:10 to reduce size\n                    g = p.grad.float().flatten()[::10].half().cpu()\n                    self.saved_grads[name] = g\n            self.opt.step(); self.opt.zero_grad(set_to_none=True); self.acc = 0\n        self.steps += 1\n        l = loss.item() * 16\n        if l < self.best_loss and not np.isnan(l): self.best_loss = l\n        if self.steps % 50 == 0: gc.collect(); torch.cuda.empty_cache()\n        return l\n    \n    def get_all_gradients(self):\n        \"\"\"Get all gradients compressed\"\"\"\n        if not self.saved_grads:\n            print(\"   âš ï¸ No gradients yet\")\n            return None\n        gc.collect()\n        \n        # Concatenate all gradients\n        all_grads = []\n        grad_info = []\n        for name, g in self.saved_grads.items():\n            all_grads.append(g.numpy())\n            grad_info.append({'name': name, 'size': len(g)})\n        \n        flat = np.concatenate(all_grads)\n        print(f\"   ðŸ“Š Compressing {len(flat):,} gradient values...\")\n        \n        # Compress\n        comp = gzip.compress(flat.tobytes(), 4)\n        \n        return {\n            'gradients': base64.b64encode(comp).decode(),\n            'info': grad_info,\n            'total_params': len(flat),\n            'compressed_size': len(comp)\n        }\n\nasync def hb(w, tps):\n    try:\n        async with aiohttp.ClientSession() as s:\n            await s.post(f\"{API}/cuda/register\", json={'wallet': w, 'clientVersion': VER, 'deviceInfo': {'gpu_name': 'T4-Colab', 'vram_gb': 15, 'hashrate': tps, 'full_model': True}})\n    except: pass\n\nasync def submit_full(w, grad_data, steps, loss):\n    \"\"\"Submit full model gradients\"\"\"\n    try:\n        payload = {\n            'wallet': w,\n            'gradients': grad_data['gradients'],\n            'gradient_info': grad_data['info'],\n            'epoch': steps,\n            'loss': loss,\n            'gpu': 'T4-Colab',\n            'full_model': True,\n            'total_params': grad_data['total_params']\n        }\n        async with aiohttp.ClientSession() as ss:\n            async with ss.post(f\"{API}/cuda/submit\", json=payload, timeout=aiohttp.ClientTimeout(total=120)) as r:\n                if r.status == 200: \n                    return (await r.json()).get('reward', 0)\n    except Exception as e:\n        print(f\"   âš ï¸ Submit error: {e}\")\n    return 0\n\nasync def run():\n    print(\"=\" * 50)\n    print(f\"  ðŸš€ AILO-1B FULL MODEL Miner v{VER}\")\n    print(\"  ðŸ“¦ Training ALL 899M parameters!\")\n    print(\"=\" * 50)\n    \n    await hb(WALLET, 0)\n    t = FullModelTrainer(); t.init()\n    \n    if not t.sync_full_model():\n        print(\"\\nâš ï¸ Could not sync - will train from scratch\")\n    \n    print(\"\\nâ›ï¸ FULL MODEL MINING STARTED!\")\n    print(f\"ðŸ“Š Dashboard: https://ailo.site/dashboard.html?wallet={WALLET[:12]}\\n\")\n    \n    rew, last_s, last_h, tps = 0.0, time.time(), time.time(), 0.0\n    skipped = 0\n    \n    try:\n        while True:\n            t0 = time.time()\n            x, y = t.batch(WALLET)\n            if x is None:\n                skipped += 1\n                if skipped % 10 == 0: print(f\"â­ï¸ Skipped {skipped} batches\")\n                await asyncio.sleep(0.5)\n                continue\n            \n            loss = t.step(x, y)\n            if loss is None: continue\n            \n            tps = 32 / (time.time() - t0 + 0.001)\n            \n            if time.time() - last_h >= 10: await hb(WALLET, tps); last_h = time.time()\n            if t.steps % 25 == 0: \n                print(f\"Step {t.steps} | Loss: {loss:.4f} | Best: {t.best_loss:.4f} | {tps:.0f} tok/s | ðŸ’° {rew:.4f} ALC\")\n            \n            # Submit every 5 minutes\n            if time.time() - last_s >= 300:\n                gc.collect(); torch.cuda.empty_cache()\n                grad_data = t.get_all_gradients()\n                \n                if grad_data and t.synced and t.best_loss < MAX_LOSS and not np.isnan(t.best_loss):\n                    print(f\"\\nðŸ“¤ Submitting ALL gradients ({grad_data['compressed_size']/1e6:.1f}MB)...\")\n                    r = await submit_full(WALLET, grad_data, t.steps, t.best_loss)\n                    rew += r\n                    print(f\"ðŸ’° +{r:.4f} ALC (Total: {rew:.4f})\")\n                elif not t.synced:\n                    print(\"\\nâ³ Not synced - training locally...\")\n                elif grad_data:\n                    print(f\"\\nâ³ Loss too high ({t.best_loss:.4f})\")\n                \n                t.sync_full_model()\n                last_s = time.time()\n                print()\n                \n    except KeyboardInterrupt: \n        print(f\"\\nâ¹ï¸ Stopped. Total earned: {rew:.4f} ALC\")\n\nawait run()"
            ],
            "metadata": {
                "id": "ailo_miner_full",
                "cellView": "form"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}