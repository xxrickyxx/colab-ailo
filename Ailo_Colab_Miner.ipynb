{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "colab": {
            "accelerator": "gpu",
            "isGpuEnabled": true,
            "isInternetEnabled": true
        }
    },
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# üöÄ AILO COLAB MINER v2.1 - FULL MODEL\n",
                "# ============================================\n",
                "# 1. Settings (‚öôÔ∏è) ‚Üí Accelerator ‚Üí GPU T4 x2 or P100\n",
                "# 2. Settings ‚Üí Internet ‚Üí ON\n",
                "# 3. Enter wallet below, then Run (Shift+Enter)\n",
                "# ============================================\n",
                "\n",
                "WALLET = \"\"  # üëà YOUR 0x... WALLET HERE\n",
                "\n",
                "# ============================================\n",
                "import subprocess, sys\n",
                "try: import aiohttp\n",
                "except: subprocess.run([sys.executable, '-m', 'pip', 'install', 'aiohttp', '-q'])\n",
                "\n",
                "import torch, torch.nn as nn, numpy as np, requests, aiohttp, asyncio, time, base64, gzip, gc, json\n",
                "\n",
                "API, SERVER, VER = \"https://ailo.site/api\", \"https://ailo.site\", \"2.1.0-colab-full\"\n",
                "MAX_LOSS = 3.5\n",
                "\n",
                "if not torch.cuda.is_available():\n",
                "    print(\"‚ùå GPU not enabled! Settings ‚Üí Accelerator ‚Üí GPU\"); raise SystemExit\n",
                "if len(WALLET) < 40:\n",
                "    print(\"‚ùå Enter wallet! Get one: https://ailo.site/wallet.html\"); raise SystemExit\n",
                "\n",
                "gpu_name = torch.cuda.get_device_name(0)\n",
                "vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
                "print(f\"‚úÖ GPU: {gpu_name} | VRAM: {vram:.1f}GB\")\n",
                "print(f\"‚úÖ Wallet: {WALLET[:12]}...\")\n",
                "\n",
                "# Model architecture matching init_full_model.py exactly\n",
                "VOCAB_SIZE, D_MODEL, N_HEADS, N_LAYERS, D_FF, MAX_SEQ = 50257, 1600, 25, 24, 6400, 512\n",
                "\n",
                "class AILO1B(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.emb = nn.Embedding(VOCAB_SIZE, D_MODEL)\n",
                "        self.pos = nn.Parameter(torch.zeros(1, MAX_SEQ, D_MODEL))\n",
                "        self.tf = nn.TransformerEncoder(\n",
                "            nn.TransformerEncoderLayer(D_MODEL, N_HEADS, D_FF, 0.1, batch_first=True), N_LAYERS)\n",
                "        self.out = nn.Linear(D_MODEL, VOCAB_SIZE)\n",
                "    def forward(self, x):\n",
                "        h = self.emb(x) * 40 + self.pos[:, :x.size(1)]\n",
                "        return self.out(self.tf(h))\n",
                "\n",
                "class FullTrainer:\n",
                "    def __init__(self):\n",
                "        self.dev = torch.device('cuda')\n",
                "        self.m = None; self.opt = None; self.steps = 0; self.acc = 0\n",
                "        self.best_loss = float('inf')\n",
                "        self.saved_grads = None\n",
                "        self.synced = False\n",
                "    \n",
                "    def init(self):\n",
                "        print(\"üß† Loading AILO-1B (899M params)...\")\n",
                "        gc.collect(); torch.cuda.empty_cache()\n",
                "        self.m = AILO1B().to(self.dev)  # Start in FP32 for stability\n",
                "        params = sum(p.numel() for p in self.m.parameters())\n",
                "        print(f\"   ‚úÖ Ready! {params:,} params | GPU: {torch.cuda.memory_allocated()/1e9:.1f}GB\")\n",
                "        # Use SGD with lower LR for stability\n",
                "        self.opt = torch.optim.SGD(self.m.parameters(), lr=5e-5, momentum=0.9)\n",
                "        self.crit = nn.CrossEntropyLoss()\n",
                "    \n",
                "    def sync_full(self):\n",
                "        \"\"\"Load full model checkpoint using state_dict approach.\"\"\"\n",
                "        try:\n",
                "            print(\"üîÑ Syncing full model (1.8GB)...\")\n",
                "            \n",
                "            # Get metadata\n",
                "            r = requests.get(f\"{SERVER}/api/model/weights\", timeout=30)\n",
                "            if r.status_code != 200: return self.sync_partial()\n",
                "            data = r.json()\n",
                "            server_loss = data.get('bestCheckpointLoss', 999)\n",
                "            \n",
                "            # Get layer info\n",
                "            info_r = requests.get(f\"{SERVER}/api/model/checkpoint/full/info\", timeout=10)\n",
                "            if info_r.status_code != 200:\n",
                "                print(\"   ‚ö†Ô∏è No full checkpoint info, using partial\")\n",
                "                return self.sync_partial()\n",
                "            \n",
                "            layers = info_r.json().get('layers', [])\n",
                "            if not layers:\n",
                "                return self.sync_partial()\n",
                "            \n",
                "            print(f\"   üì• Downloading full checkpoint (loss: {server_loss})...\")\n",
                "            cr = requests.get(f\"{SERVER}/api/model/checkpoint/full\", timeout=300, stream=True)\n",
                "            if cr.status_code != 200:\n",
                "                return self.sync_partial()\n",
                "            \n",
                "            weights = np.frombuffer(cr.content, dtype=np.float16)\n",
                "            print(f\"   üì¶ Downloaded {len(weights):,} params\")\n",
                "            \n",
                "            # Build state dict from checkpoint\n",
                "            state_dict = {}\n",
                "            loaded = 0\n",
                "            \n",
                "            with torch.no_grad():\n",
                "                for layer in layers:\n",
                "                    name = layer['name']\n",
                "                    offset = layer['offset']\n",
                "                    size = layer['size']\n",
                "                    shape = tuple(layer['shape'])\n",
                "                    \n",
                "                    w = weights[offset:offset+size].astype(np.float32)\n",
                "                    if len(w) != size:\n",
                "                        print(f\"   ‚ö†Ô∏è Size mismatch for {name}\")\n",
                "                        continue\n",
                "                    \n",
                "                    # Check for NaN/Inf\n",
                "                    if np.isnan(w).any() or np.isinf(w).any():\n",
                "                        print(f\"   ‚ö†Ô∏è Bad values in {name}, skipping\")\n",
                "                        continue\n",
                "                    \n",
                "                    tensor = torch.from_numpy(w).reshape(shape)\n",
                "                    state_dict[name] = tensor\n",
                "                    loaded += 1\n",
                "            \n",
                "            print(f\"   üì¶ Built state_dict with {loaded} layers\")\n",
                "            \n",
                "            # Load using PyTorch's load_state_dict\n",
                "            result = self.m.load_state_dict(state_dict, strict=False)\n",
                "            print(f\"   Loaded: {len(result.unexpected_keys)} unexpected, {len(result.missing_keys)} missing\")\n",
                "            \n",
                "            if loaded > 0:\n",
                "                self.synced = True\n",
                "                self.best_loss = float(server_loss) if isinstance(server_loss, (int, float)) else 3.5\n",
                "                print(f\"   ‚úÖ Full model loaded! Target loss: {self.best_loss:.4f}\")\n",
                "                return True\n",
                "            \n",
                "            return self.sync_partial()\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"   ‚ö†Ô∏è Full sync failed: {e}\")\n",
                "            return self.sync_partial()\n",
                "    \n",
                "    def sync_partial(self):\n",
                "        \"\"\"Fallback: load only out.weight.\"\"\"\n",
                "        try:\n",
                "            print(\"   üì• Falling back to partial checkpoint...\")\n",
                "            r = requests.get(f\"{SERVER}/api/model/weights\", timeout=30)\n",
                "            if r.status_code != 200: return False\n",
                "            data = r.json()\n",
                "            server_loss = data.get('bestCheckpointLoss', 999)\n",
                "            \n",
                "            cr = requests.get(f\"{SERVER}/api/model/checkpoint\", timeout=60)\n",
                "            if cr.status_code != 200: return False\n",
                "            \n",
                "            w = np.frombuffer(cr.content, dtype=np.float16).astype(np.float32)\n",
                "            with torch.no_grad():\n",
                "                exp = self.m.out.weight.numel()\n",
                "                if len(w) >= exp:\n",
                "                    self.m.out.weight.data = torch.from_numpy(w[:exp]).reshape_as(self.m.out.weight).to(self.dev)\n",
                "                    self.synced = True\n",
                "                    self.best_loss = float(server_loss) if isinstance(server_loss, (int, float)) else 3.5\n",
                "                    print(f\"   ‚úÖ Partial model loaded ({exp:,} weights)\")\n",
                "                    return True\n",
                "            return False\n",
                "        except Exception as e:\n",
                "            print(f\"   ‚ö†Ô∏è Partial sync failed: {e}\")\n",
                "            return False\n",
                "    \n",
                "    def batch(self, w):\n",
                "        try:\n",
                "            r = requests.get(f\"{API}/cuda/training-data\", params={'batchSize': 1, 'wallet': w}, timeout=10)\n",
                "            if r.ok:\n",
                "                t = r.json().get('articles', [''])[0]\n",
                "                if len(t) >= 33:\n",
                "                    tk = [ord(c) % VOCAB_SIZE for c in t[:33]]\n",
                "                    return torch.tensor([tk[:32]], dtype=torch.long), torch.tensor([tk[1:33]], dtype=torch.long)\n",
                "        except: pass\n",
                "        return None, None\n",
                "    \n",
                "    def step(self, x, y):\n",
                "        if x is None: return None\n",
                "        self.m.train()\n",
                "        x, y = x.to(self.dev), y.to(self.dev)\n",
                "        \n",
                "        # No autocast - keep FP32 for stability\n",
                "        out = self.m(x)\n",
                "        loss = self.crit(out.view(-1, VOCAB_SIZE), y.view(-1)) / 16\n",
                "        \n",
                "        # Check for NaN before backprop\n",
                "        if torch.isnan(loss) or torch.isinf(loss):\n",
                "            return None\n",
                "        \n",
                "        loss.backward()\n",
                "        self.acc += 1\n",
                "        \n",
                "        if self.acc >= 16:\n",
                "            nn.utils.clip_grad_norm_(self.m.parameters(), 0.5)  # Tighter clipping\n",
                "            g = self.m.out.weight.grad\n",
                "            if g is not None and not torch.isnan(g).any():\n",
                "                self.saved_grads = g.clone().cpu()\n",
                "            self.opt.step()\n",
                "            self.opt.zero_grad(set_to_none=True)\n",
                "            self.acc = 0\n",
                "        \n",
                "        self.steps += 1\n",
                "        l = loss.item() * 16\n",
                "        if not np.isnan(l) and l < self.best_loss:\n",
                "            self.best_loss = l\n",
                "        if self.steps % 50 == 0:\n",
                "            gc.collect(); torch.cuda.empty_cache()\n",
                "        return l\n",
                "    \n",
                "    def grads(self):\n",
                "        g = self.saved_grads\n",
                "        if g is None: return None\n",
                "        gc.collect()\n",
                "        flat = g.float().flatten()\n",
                "        sample = flat[::10]\n",
                "        comp = gzip.compress(sample.half().numpy().tobytes(), 4)\n",
                "        return base64.b64encode(comp).decode()\n",
                "\n",
                "async def hb(w, tps):\n",
                "    try:\n",
                "        async with aiohttp.ClientSession() as s:\n",
                "            await s.post(f\"{API}/cuda/register\", json={'wallet': w, 'clientVersion': VER, 'deviceInfo': {'gpu_name': f'{gpu_name}-Colab', 'vram_gb': vram, 'hashrate': tps}})\n",
                "    except: pass\n",
                "\n",
                "async def submit(w, g, s, l):\n",
                "    try:\n",
                "        async with aiohttp.ClientSession() as ss:\n",
                "            async with ss.post(f\"{API}/cuda/submit\", json={'wallet': w, 'gradients': g, 'epoch': s, 'loss': l, 'gpu': f'{gpu_name}-Colab'}) as r:\n",
                "                if r.status == 200: return (await r.json()).get('reward', 0)\n",
                "    except: pass\n",
                "    return 0\n",
                "\n",
                "async def run():\n",
                "    print(\"=\" * 50)\n",
                "    print(f\"  üöÄ AILO COLAB FULL MODEL v{VER}\")\n",
                "    print(f\"  üì¶ Training ALL 899M parameters!\")\n",
                "    print(\"=\" * 50)\n",
                "    \n",
                "    await hb(WALLET, 0)\n",
                "    t = FullTrainer(); t.init()\n",
                "    \n",
                "    if not t.sync_full():\n",
                "        print(\"\\n‚ö†Ô∏è Could not sync with global model\")\n",
                "    \n",
                "    print(\"\\n‚õèÔ∏è MINING STARTED!\")\n",
                "    print(f\"üìä Dashboard: https://ailo.site/dashboard.html?wallet={WALLET[:12]}\\n\")\n",
                "    \n",
                "    rew, last_s, last_h, tps = 0.0, time.time(), time.time(), 0.0\n",
                "    nan_count = 0\n",
                "    \n",
                "    try:\n",
                "        while True:\n",
                "            t0 = time.time()\n",
                "            x, y = t.batch(WALLET)\n",
                "            if x is None:\n",
                "                await asyncio.sleep(0.5)\n",
                "                continue\n",
                "            \n",
                "            loss = t.step(x, y)\n",
                "            if loss is None:\n",
                "                nan_count += 1\n",
                "                if nan_count > 10:\n",
                "                    print(\"‚ö†Ô∏è Too many NaN, re-syncing...\")\n",
                "                    t.sync_partial()  # Fallback to partial\n",
                "                    nan_count = 0\n",
                "                continue\n",
                "            nan_count = 0\n",
                "            \n",
                "            tps = 32 / (time.time() - t0 + 0.001)\n",
                "            \n",
                "            if time.time() - last_h >= 10: await hb(WALLET, tps); last_h = time.time()\n",
                "            if t.steps % 25 == 0: print(f\"Step {t.steps} | Loss: {loss:.4f} | Best: {t.best_loss:.4f} | {tps:.0f} tok/s | üí∞ {rew:.4f}\")\n",
                "            \n",
                "            if time.time() - last_s >= 300:\n",
                "                gc.collect(); torch.cuda.empty_cache()\n",
                "                g = t.grads()\n",
                "                if g and t.synced and t.best_loss < MAX_LOSS and not np.isnan(t.best_loss):\n",
                "                    print(f\"\\nüì§ Submitting gradients (loss: {t.best_loss:.4f})...\")\n",
                "                    r = await submit(WALLET, g, t.steps, t.best_loss)\n",
                "                    rew += r; print(f\"üí∞ +{r:.4f} (Total: {rew:.4f})\")\n",
                "                t.sync_partial()  # Re-sync with latest\n",
                "                last_s = time.time()\n",
                "                print()\n",
                "    except KeyboardInterrupt:\n",
                "        print(f\"\\n‚èπÔ∏è Stopped. Total: {rew:.4f} ALC\")\n",
                "\n",
                "await run()"
            ]
        }
    ]
}