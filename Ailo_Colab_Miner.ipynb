{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# üöÄ Ailo Network - Colab Miner (AILO-1B)\n\n**Mine AiloCoin with Google Colab's free T4 GPU!**\n\n1. Enable GPU: Runtime > Change runtime type > T4\n2. Enter wallet in cell 2\n3. Runtime > Run all"
            ],
            "metadata": {
                "id": "header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "#@title ‚öôÔ∏è 1. Setup\n!pip install torch --index-url https://download.pytorch.org/whl/cu118 -q\n!pip install aiohttp requests -q\n\nimport gc\ngc.collect()\n\nimport torch\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"‚úÖ VRAM: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f} GB\")\nelse:\n    print(\"‚ùå Enable GPU: Runtime > Change runtime type > T4\")"
            ],
            "metadata": {
                "id": "setup"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title üîë 2. Wallet\nWALLET = \"\"  #@param {type:\"string\"}\nif len(WALLET) < 40:\n    print(\"‚ùå Enter wallet from https://ailo.site/wallet.html\")\nelse:\n    print(f\"‚úÖ Wallet: {WALLET[:12]}...{WALLET[-8:]}\")"
            ],
            "metadata": {
                "id": "wallet"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title üß† 3. AILO-1B Model\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport requests\nimport aiohttp\nimport asyncio\nimport time\nimport base64\nimport gzip\nimport gc\n\nAPI = \"https://ailo.site/api\"\nVER = \"1.3.0-colab\"\nBATCH = 1\nSEQ = 32\nACCUM = 16\nSUBMIT_SEC = 300\n\nclass AILO1B(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # AILO-1B: 24 layers, 1600d, 25 heads = 899M params\n        self.emb = nn.Embedding(50257, 1600)\n        self.pos = nn.Parameter(torch.zeros(1, 512, 1600))\n        layer = nn.TransformerEncoderLayer(d_model=1600, nhead=25, dim_feedforward=6400, dropout=0.1, batch_first=True)\n        self.tf = nn.TransformerEncoder(layer, num_layers=24)\n        self.out = nn.Linear(1600, 50257)\n\n    def forward(self, x):\n        x = self.emb(x) * 40.0 + self.pos[:, :x.size(1), :]\n        return self.out(self.tf(x))\n\nclass Trainer:\n    def __init__(self):\n        self.dev = torch.device('cuda')\n        self.model = None\n        self.opt = None\n        self.crit = nn.CrossEntropyLoss()\n        self.steps = 0\n        self.acc = 0\n\n    def init(self):\n        print(\"üß† Loading AILO-1B...\")\n        gc.collect()\n        torch.cuda.empty_cache()\n        \n        # Create model directly in FP16 on GPU to save RAM\n        with torch.cuda.amp.autocast():\n            self.model = AILO1B().half().to(self.dev)\n        \n        p = sum(x.numel() for x in self.model.parameters())\n        print(f\"   üìê {p:,} params ({p/1e9:.2f}B)\")\n        print(f\"   üíæ GPU: {torch.cuda.memory_allocated()/1024**3:.1f} GB\")\n        \n        # SGD uses less RAM than AdamW (no momentum buffers)\n        self.opt = torch.optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n        print(\"   ‚úÖ Ready!\")\n\n    def batch(self, w):\n        try:\n            r = requests.get(f\"https://ailo.site/api/cuda/training-data\", params={'batchSize': BATCH, 'wallet': w}, timeout=10)\n            if r.ok:\n                texts = r.json().get('articles', [])\n                if texts:\n                    bx, by = [], []\n                    for t in texts:\n                        tk = [ord(c) % 50257 for c in t[:SEQ+1]]\n                        tk += [0] * (SEQ+1 - len(tk))\n                        bx.append(tk[:SEQ])\n                        by.append(tk[1:SEQ+1])\n                    return torch.tensor(bx), torch.tensor(by)\n        except: pass\n        return torch.randint(0, 50257, (BATCH, SEQ)), torch.randint(0, 50257, (BATCH, SEQ))\n\n    def step(self, x, y):\n        self.model.train()\n        x, y = x.to(self.dev), y.to(self.dev)\n        \n        with torch.cuda.amp.autocast():\n            o = self.model(x)\n            loss = self.crit(o.view(-1, 50257), y.view(-1)) / ACCUM\n        \n        loss.backward()\n        self.acc += 1\n        \n        if self.acc >= ACCUM:\n            nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n            self.opt.step()\n            self.opt.zero_grad(set_to_none=True)\n            self.acc = 0\n        \n        self.steps += 1\n        if self.steps % 100 == 0:\n            torch.cuda.empty_cache()\n        \n        return loss.item() * ACCUM\n\n    def grads(self):\n        g = [p.grad.cpu().float().flatten() for p in self.model.parameters() if p.grad is not None]\n        if not g: return None\n        return base64.b64encode(gzip.compress(torch.cat(g).half().numpy().tobytes(), 4)).decode()\n\nprint(\"‚úÖ AILO-1B defined\")"
            ],
            "metadata": {
                "id": "model"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title üöÄ 4. Mine!\n\nasync def submit(w, g, s, l):\n    try:\n        async with aiohttp.ClientSession() as sess:\n            async with sess.post(f\"{API}/cuda/submit\", json={'wallet': w, 'gradients': g, 'epoch': s, 'loss': l, 'gpu': 'T4-Colab'}) as r:\n                if r.status == 200:\n                    return (await r.json()).get('reward', 0)\n    except: pass\n    return 0\n\nasync def reg(w):\n    try:\n        async with aiohttp.ClientSession() as s:\n            await s.post(f\"{API}/cuda/register\", json={'wallet': w, 'clientVersion': VER, 'deviceInfo': {'gpu_name': 'T4-Colab', 'vram_gb': 15}})\n    except: pass\n\nasync def run():\n    if len(WALLET) < 40:\n        print(\"‚ùå Set wallet!\")\n        return\n\n    print(\"=\"*40)\n    print(f\"  AILO-1B Colab Miner v{VER}\")\n    print(\"=\"*40)\n\n    await reg(WALLET)\n    print(\"üìù Registered\")\n\n    t = Trainer()\n    t.init()\n\n    print(\"\\n‚õèÔ∏è MINING!\\n\")\n\n    rewards = 0.0\n    last = time.time()\n\n    try:\n        while True:\n            t0 = time.time()\n            x, y = t.batch(WALLET)\n            loss = t.step(x, y)\n            tps = (BATCH * SEQ) / (time.time() - t0)\n\n            if t.steps % 25 == 0:\n                print(f\"Step {t.steps:4d} | Loss: {loss:.4f} | {tps:.0f} tok/s | GPU: {torch.cuda.memory_allocated()/1024**3:.1f}GB\")\n\n            if time.time() - last >= SUBMIT_SEC:\n                print(\"\\nüì§ Submitting...\")\n                g = t.grads()\n                if g:\n                    r = await submit(WALLET, g, t.steps, loss)\n                    rewards += r\n                    print(f\"üí∞ +{r:.4f} ALC (Total: {rewards:.4f})\\n\")\n                last = time.time()\n\n    except KeyboardInterrupt:\n        print(f\"\\n‚èπÔ∏è Done. Total: {rewards:.4f} ALC\")\n\nawait run()"
            ],
            "metadata": {
                "id": "run"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}